# Bilingual RAG Q&A System

A Retrievalâ€‘Augmented Generation (RAG) system that accepts Bangla or English questions and answers them based on the **HSC26 Bangla 1stâ€‘Paper** PDF.  
Built with free, openâ€‘source tools for fully offline use.

---

## ğŸ“– Project Overview

- **Input**: User question (Bangla or English)  
- **Retrieval**: FAISS vector index over PDFâ€‘derived chunks  
- **Generation**: Local LLM (Mistral via Ollama)  
- **Shortâ€‘Term Memory**: Recent chat history  
- **Longâ€‘Term Memory**: PDF corpus in FAISS  

---

## ğŸš€ Features

- âœ… **Bilingual** (Bangla & English) question answering  
- âœ… **OCR**â€‘backed PDF ingestion (via Tesseract + Poppler)  
- âœ… **Semantic retrieval** using FAISS & multilingual embeddings  
- âœ… **Contextâ€‘strict prompting** for grounded answers  
- âœ… **Streamlit UI** & **FastAPI** backend  
- âœ… Fully **local/offline**â€”no paid APIs required  

---

## ğŸ“‚ Directory Structure
```
RAG-system-for-10MS/
â”‚
â”œâ”€â”€ app/                         # Core application modules
â”‚   â”œâ”€â”€ __init__.py              # Marks this as a Python package
â”‚   â”œâ”€â”€ pdf_loader.py            # PDF â†’ clean text â†’ chunks
â”‚   â”œâ”€â”€ retriever.py             # Build & load FAISS index
â”‚   â”œâ”€â”€ generator.py             # Ollama/Mistral invocation
â”‚   â”œâ”€â”€ memory.py                # Shortâ€‘term chat memory
â”‚   â”œâ”€â”€ main.py                  # FastAPI app (/ask endpoint)
â”‚   â”œâ”€â”€ streamlit_app.py         # Streamlit UI
â”‚   â””â”€â”€ requirements.txt         # Python deps for app/
â”‚
â”œâ”€â”€ data/                        
â”‚   â””â”€â”€ HSC26-Bangla1st-Paper.pdf # Input PDF
â”‚
â”œâ”€â”€ faiss_index/                 # (autoâ€‘generated) FAISS files
â”œâ”€â”€ evaluation/                  # RAG evaluation scripts
â”‚   â””â”€â”€ evaluate_rag.py
â”œâ”€â”€ Dockerfile                   # Container spec
â”œâ”€â”€ docker-compose.yml           # Service orchestration
â””â”€â”€ README.md                    # â† You are here
```
--- 

# ğŸ›  Prerequisites
- PythonÂ 3.10+

- TesseractÂ OCR

- Windows: install via .exe and add to PATH

- Linux/macOS: sudo apt install tesseract-ocr tesseract-ocr-ben

- Poppler (for pdf2image)

- Windows: download/extract & add Library\bin to PATH

- Linux/macOS: sudo apt install poppler-utils

- Ollama (for local Mistral model)

- Install instructions

- Run ollama run mistral in the background
---


# âš™ï¸ Installation
## 1. Clone the repo :
```
git clone https://github.com/Sabuj-Majumder/RAG-system-for-10MS
```
## 2. Create & activate a virtual environment
```
python -m venv venv
# Windows
.\venv\Scripts\Activate
# macOS/Linux
source venv/bin/activate
```
## 3.Install dependencies
```
pip install --upgrade pip
pip install -r app/requirements.txt
```
## 4.Verify tools
```
tesseract --version
pdftoppm -v       # Poppler tool
ollama run mistral # start Mistral model
```
---

# â–¶ï¸ Running Locally:
## 1. Start the FastAPI backend
```
uvicorn app.main:app --reload
```
- Endpoint: POST http://localhost:8000/ask

- Request:
```
  { "question": "à¦…à¦¨à§à¦ªà¦®à§‡à¦° à¦­à¦¾à¦·à¦¾à¦¯à¦¼ à¦¸à§à¦ªà§à¦°à§à¦· à¦•à¦¾à¦•à§‡ à¦¬à¦²à¦¾ à¦¹à¦¯à¦¼à§‡à¦›à§‡?" }
```
- Response:
```
{
  "answer": "à¦¶à§à¦®à§à¦­à§à¦¨à¦¾à¦¥",
  "sources": [
    { "question_id": "à§§." },
    ...
  ]
}
```
---

# 2. Launch the Streamlit UI
## In another terminal:
```
streamlit run app/streamlit_app.py
```
Open your browser at http://localhost:8501 to ask questions interactively.
# ğŸ³ Docker Deployment (Optional)
## Build & run both services via Docker Compose:
```
docker-compose up --build

```
- FastAPI â†’ http://localhost:8000

- Streamlit UI â†’ http://localhost:8501

---

# ğŸ”§ Configuration

- PDF path: in app/main.py (constant PDF_PATH)

- Tesseract: adjust pytesseract.pytesseract.tesseract_cmd in app/pdf_loader.py

- Poppler: adjust POPPLER_PATH in app/pdf_loader.py

- Embedding model: change model_name in app/retriever.py

- LLM & temperature: change "--temperature", "0" in app/generator.py

---


# ğŸ§ª RAG Evaluation
We provide a simple evaluation script to measure Relevance and Groundedness.

## 1. Define test cases
Edit evaluation/evaluate_rag.py to add your own queries, expected answers, and expected questionâ€‘IDs.

## 2. Run the evaluation
```
python evaluation/evaluate_rag.py
```
Metrics reported per case:

- Relevance Score: Cosine similarity between the query and retrieved chunk

- Chunk Match: Whether the retrieved chunkâ€™s ID matches the expected ID

- Generated Answer: The modelâ€™s response

- Answer Correctness: Whether the expected answer string appears in the generated answer

- Groundedness Score: Cosine similarity between the generated answer and the retrieved context

## ample output:
```
----------------------------------------
Query: à¦…à¦¨à§à¦ªà¦®à§‡à¦° à¦­à¦¾à¦·à¦¾à¦¯à¦¼ à¦¸à§à¦ªà§à¦°à§à¦· à¦•à¦¾à¦•à§‡ à¦¬à¦²à¦¾ à¦¹à¦¯à¦¼à§‡à¦›à§‡?
Expected chunk: à§§. | Retrieved: à§§. (Relevance=0.72)
Expected answer: à¦¶à§à¦®à§à¦­à§à¦¨à¦¾à¦¥
Generated answer: à¦¶à§à¦®à§à¦­à§à¦¨à¦¾à¦¥
Answer correct? True, Groundedness=0.65
...
Overall metrics:
  Avg. Relevance:    0.68
  Avg. Groundedness: 0.61
```
---
# ğŸ›  Troubleshooting
## Import errors (ModuleNotFoundError)
- Ensure you run Streamlit & Uvicorn from the project root, and that app/__init__.py exists.

## OCR garbage output
- Tweak DPI (600Â â†’Â 800), PSM mode (--psmÂ 3), or add custom corrections in app/pdf_loader.py.

## Wrong retrieval
- Increase k in get_retriever().as_retriever(k=6) or adjust chunk boundaries.

## Hallucinations
- Use zero temperature (--temperatureÂ 0) and strict â€œAnswer only from contextâ€ prompts.
